---
title: "Data 621 Homework 3"
author: "Sung Lee"
date: "10/21/2021"
output: 
  html_document:
    code_folding: show
    df_print: paged
    smooth_scroll: false
number_sections: true
theme: paper
---

# Introduction

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).  

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

Variable | Description
---------|-------------------------
zn | proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
indus | proportion of non-retail business acres per suburb (predictor variable)
chas | a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
nox | nitrogen oxides concentration (parts per 10 million) (predictor variable)
rm | average number of rooms per dwelling (predictor variable)
age | proportion of owner-occupied units built prior to 1940 (predictor variable)
dis | weighted mean of distances to five Boston employment centers (predictor variable)
rad | index of accessibility to radial highways (predictor variable)
tax | full-value property-tax rate per $10,000 (predictor variable)
ptratio | pupil-teacher ratio by town (predictor variable)
lstat | lower status of the population (percent) (predictor variable)
medv | median value of owner-occupied homes in $1000s (predictor variable)
target | whether the crime rate is above the median crime rate (1) or not (0) (response variable)

```{r, echo=FALSE}
library(pastecs)
library(tidyverse)
# https://www.rdocumentation.org/packages/naniar/versions/0.6.1
library(naniar)
library(reshape2)
library(ggplot2)

# Used for skewness
library(moments)

# Log Scale
library(scales)

# Correlation corrplot
# https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
library(corrplot)

# https://www.rdocumentation.org/packages/car/versions/3.0-11/topics/vif
library(car)

library(caret)
library(leaps)
library(pROC)
```

# Data Exploration  

```{r, echo=FALSE}
# To help prevent scientific notation for viewed values
options(scipen=100)

# To set the number of decimal places
options(digits=4)

# From http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


# Reading in the csv files
trainData <- read.csv("https://raw.githubusercontent.com/logicalschema/Fall-2021/main/DATA621/hw3/crime-training-data_modified.csv")

evalData <- read.csv("https://raw.githubusercontent.com/logicalschema/Fall-2021/main/DATA621/hw3/crime-evaluation-data_modified.csv")

```

The following is a snippet of the training data along with summary and other statistics:

```{r, echo=FALSE}
head(trainData)
summary(trainData)

# https://cran.r-project.org/web/packages/pastecs/pastecs.pdf
stat.desc(trainData, basic = FALSE)
```  

There are no missing values in the training data. As noted above, `target` is the dependent variable with 12 other variables with which we can decide to build the model.  

## Distributions  

The following are the distributions for the variables:

```{r, echo=FALSE}
par(mfrow = c(3, 3))

plotData <- melt(trainData)

ggplot(plotData, aes(x= value)) + 
  theme(panel.border = element_blank(), panel.background = element_blank(), 
        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_density(fill='dodgerblue') + facet_wrap(~variable, scales = 'free') 

```  

Here are some general observations:

The variable `zn` (proportion of residential land zoned for large lots) is small and a log transformation is needed. For the variable `chas` there is a skew of hourses not bounding the Charles River and probably not good as a predictor variable.


## Skewness  

The following looks at the skewness of each of the variables:  

```{r, echo=FALSE}
skewness(trainData)
```  


## Boxplots  

The following is a plot of the boxplots for the variables:  

```{r, echo=FALSE}
p1 <- ggplot(trainData) + 
  aes(y =zn, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p2 <- ggplot(trainData) + 
  aes(y =indus, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p3 <- ggplot(trainData) + 
  aes(y =chas, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p4 <- ggplot(trainData) + 
  aes(y =nox, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p5 <- ggplot(trainData) + 
  aes(y =rm, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p6 <- ggplot(trainData) + 
  aes(y =age, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p7 <- ggplot(trainData) + 
  aes(y =dis, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p8 <- ggplot(trainData) + 
  aes(y =rad, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p9 <- ggplot(trainData) + 
  aes(y =tax, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p10 <- ggplot(trainData) + 
  aes(y =ptratio, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p11 <- ggplot(trainData) + 
  aes(y =lstat, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")

p12 <- ggplot(trainData) + 
  aes(y =medv, x = as.factor(target)) +
  geom_boxplot(fill="dodgerblue", alpha=0.3) +
  theme_minimal() + xlab("target")


multiplot(p1, p2, p3, p4, p5, p6, cols=3)
multiplot(p7, p8, p9, p10, p11, p12, cols=3)


```

## Correlation  

The following constructs a correlation matrix with heatmap. This matrix measures linear relationships between the variables. These values range from -1 to 1. If the value is close to -1, there is a strong negative correlation between the variables. If there is a value close to 1, there is a strong positive correlation between the variables.


```{r, echo=FALSE}
trainData %>%
        complete.cases() %>% 
        trainData[., ] %>%                         
        cor() %>%
        corrplot(method = "number")

```


```{r, echo=FALSE}
cor(trainData)

```  

The variables that have a strong correlation with `target`. The variables that have high correlations with `target` are: `nox` (0.73), `age` (0.63), `tax` (0.61), `indus` (0.60), `dis` (-0.62), and `logZN` (-0.47).  


# Data Preparation  

The variable `zn` will undergo a log transformation.

```{r, echo=FALSE}
trainData$logZN <- log1p(trainData$zn)
hist(trainData$logZN)
```


## Collinearity  

The VIF (variance inflation factors): "Calculates variance-inflation and generalized variance-inflation factors (VIFs and GVIFs) for linear, generalized linear, and other regression models." If the VIF = 1, then not correlated. If the VIF is in the range (1,5), it is moderately correlated. If the VID is greater than 5, the predictors are highly correlated. Overall, vif, helps to identify collinearity. 

```{r, echo=FALSE}
mask <- names(trainData) %in% c('zn')
lmfit <- lm(target ~ ., data=trainData[!mask])
vif(lmfit)
```

## Splitting  

I split the training data into two partitions.


```{r, echo=FALSE}
set.seed(102621)
trainIndex <- createDataPartition(trainData$target, p = 0.8, 
                                  list = FALSE)
training <- trainData[trainIndex, ]
evaluation <- trainData[-trainIndex, ]

head(training)
head(evaluation)
```  



# Models  

In developing the models, I used the `leaps` [package](https://cran.r-project.org/web/packages/leaps/leaps.pdf). According to the documentation, "leaps() performs an exhaustive search for the best subsets of the variables in x for predicting y in linear regression, using an efficient branch-and-bound algorithm."  



```{r, echo=FALSE}
y <- training$target
x <- model.matrix(target~ . - 1 - zn, data=training)
best_mods <- leaps(x, y, nbest=1)
best_mods

# Print the minimum Mallow's CP
min(best_mods$Cp)

```

The best variable selection using the lowest Mallow's Cp  value is #7 with Cp value of 5.4.  

**7  FALSE FALSE TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE**  


```{r, echo=FALSE}
mask <- names(training) %in% c('zn', 'target')
names(trainData[!mask])
```  

Variable | Present
---------|-------------  
indus | FALSE  
chas | FALSE  
nox | TRUE  
rm | FALSE  
age | TRUE  
dis | FALSE  
rad | TRUE  
tax | TRUE  
ptratio | FALSE  
lstat | TRUE  
medv | TRUE  
logZN | TRUE  

The variables to use are `nox`, `age`, `rad`, `tax`, `lstat`, `medv`, and `logZN`.

# Build Models  


## 1st Model
Using the information about the model with the lowest Mallow's Cp value, I build a model using the seven variables. Here is a summary of the model:

```{r, echo=FALSE}
reduced_model <- glm(target ~ nox + age + rad + tax + lstat + medv + logZN, data=training, family = 'binomial')
summary(reduced_model)

```  

Now, let's look at how the model works with our test data. 


```{r, echo=FALSE}
predictionProbabilities <- predict(reduced_model, evaluation, type="response")



actuals_predictions <- data.frame(cbind(actual=evaluation$target, predictedProb=predictionProbabilities))

actuals_predictions$prediction <- ifelse(actuals_predictions$predictedProb < 0.5, 0, 1)



head(actuals_predictions)
confusionMatrix(as.factor(actuals_predictions$prediction), as.factor(actuals_predictions$actual), positive = '1')

```


```{r, echo=FALSE}
proc = roc(actuals_predictions$actual, actuals_predictions$prediction)
plot(proc)

```

```{r, echo=FALSE}
print(proc$auc)
```

## 2nd Model  

This model is the kitchen sink, where all the variables will be used.  

```{r, echo=FALSE}

model <- glm(target ~ indus + nox + rm + age + dis + rad + tax + ptratio + lstat + medv + logZN, data=training, family = 'binomial')
summary(model)

predictionProbabilities <- predict(model, evaluation, type="response")



actuals_predictions <- data.frame(cbind(actual=evaluation$target, predictedProb=predictionProbabilities))

actuals_predictions$prediction <- ifelse(actuals_predictions$predictedProb < 0.5, 0, 1)



head(actuals_predictions)
confusionMatrix(as.factor(actuals_predictions$prediction), as.factor(actuals_predictions$actual), positive = '1')

```


```{r, echo=FALSE}
proc = roc(actuals_predictions$actual, actuals_predictions$prediction)
plot(proc)

```

```{r, echo=FALSE}
print(proc$auc)
```


## 3rd Model  

I will use these variables with high correlations: nox (0.73), age (0.63), tax (0.61), indus (0.60),  dis (-0.62), and logZN (-0.47).


```{r, echo=FALSE}

model3 <- glm(target ~ nox + age + tax + indus + dis + logZN, data=training, family = 'binomial')
summary(model)

predictionProbabilities <- predict(model3, evaluation, type="response")


actuals_predictions <- data.frame(cbind(actual=evaluation$target, predictedProb=predictionProbabilities))

actuals_predictions$prediction <- ifelse(actuals_predictions$predictedProb < 0.5, 0, 1)



head(actuals_predictions)
confusionMatrix(as.factor(actuals_predictions$prediction), as.factor(actuals_predictions$actual), positive = '1')

```


```{r, echo=FALSE}
proc = roc(actuals_predictions$actual, actuals_predictions$prediction)
plot(proc)

```

```{r, echo=FALSE}
print(proc$auc)
```

# Model Selection  

Surprisingly, I went with the 2nd model because it had a better accuracy than the others.

## Model and Evaluation Data  

The model selected is applied to the evaluation data and stored in a column `prediction`.

```{r, echo=FALSE}
evalData$logZN <- log1p(evalData$zn)

predictions <- predict(model, evalData, type="response")
evalData$prediction <- ifelse(predictions < 0.5, 0, 1)


head(evalData)
```















